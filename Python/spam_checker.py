import logging
from dotenv import load_dotenv
import os
from langchain_ollama import ChatOllama
from langchain_core.messages import HumanMessage

load_dotenv()

LOCAL_LLM = os.getenv('LOCAL_LLM')
print(f'LOCAL_LLM ={LOCAL_LLM }')

logger = logging.getLogger(__name__)

async def check_spam(topic: str) -> bool:
    logger.info(f"topic= {topic}")
    prompt = """Ты — высокоточная система детекции спама для Telegram. Анализируй сообщения строго по нижеуказанным правилам.

### Критерии СПАМА (отвечать "SPAM"):
1. **Финансовые обещания**
   - Любые конкретные суммы ("350$", "от 500$", "2000$ в неделю")
   - Указание временного периода ("в день", "ежедневно", "за неделю")
   - Фразы: "доход от", "прибыль", "заработок", "получайте"

2. **Скрытое взаимодействие**
   - Прямые призывы: "пишите в ЛС", "в сообщения", "напишите мне"
   - Косвенные призывы: "заинтересованы?", "интересно?", "хотите узнать?"
   - Требование действий: "оставьте +", "напишите 'старт'"

3. **Расплывчатые предложения**
   - "Отличный формат", "интересное предложение", "выгодные условия"
   - "Сотрудничество", "удалённая работа" без конкретики
   - "Достойный доход", "хороший заработок" без деталей

4. **Структурные маркеры**
   - Сочетание финансовых цифр + вопроса + призыва к действию
   - Использование точек/тире вместо нормальных предложений

### Исключения (отвечать "NOT_SPAM"):
1. **Техническая лексика**
   - AI/ИИ, программирование (Python, C++), нейросети
   - Оборудование (GPU, CPU, сервера), фреймворки
   - Профессиональные термины ("инференс", "трансформер")

2. **Нормальная коммуникация**
   - Вопросы/обсуждения без финансового подтекста
   - Сообщения с конкретной технической информацией
   - Профессиональные дискуссии любого рода

### Особые указания:
1. Любое сочетание "конкретная сумма + призыв к действию" = SPAM
2. Технические термины перевешивают спам-маркеры
3. Короткие сообщения ("GPUStack") не считаются спамом

Формат ответа ТОЛЬКО:
SPAM
ИЛИ
NOT_SPAM

Анализируемое сообщение:
{question}"""

    try:

        llm = ChatOllama(model=LOCAL_LLM, temperature=0)

        # Формирование запроса для LLM
        print(f'text={topic}')
        prompt_formatted = prompt.format(question=topic)
        # Отправка запроса (сформированного промпта) в модель и получение ответа
        generation = llm.invoke([HumanMessage(content=prompt_formatted)])
        model_response = generation.content
        print(f'model_response={model_response}')
        print(type(model_response))


        return model_response == "SPAM"
    except Exception as e:
        logger.error(f"Ошибка при проверке на спам: {e}")
        return False